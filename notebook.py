# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CkfbRTbu6ffOaPddVooTn5SZJcd41jbt

SUBMISSION PREDICTIVE ANALYTYC - PREDIKSI STROKE

original dataset: https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset

# Import Library
"""

!pip freeze > requirements.txt
from google.colab import files
files.download('requirements.txt')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

"""Mengimport semua library yang dibutuhkan

# Load Data
"""

df = pd.read_csv('/content/drive/MyDrive/SPK/DBS/predictive analystic/healthcare-dataset-stroke-data.csv')
df

"""Menampilkan seluruh data, data diawal terdapat 5110 data

# Exploratory Data Analysis (EDA)
"""

df.info()

"""Dataset ini berisi 5110 entri dengan 12 kolom yang merepresentasikan data kesehatan dan gaya hidup individu terkait risiko stroke. Kolom id merupakan identifier unik setiap data, sementara gender, ever_married, work_type, Residence_type, dan smoking_status adalah fitur kategorikal bertipe objek yang menggambarkan karakteristik demografis dan kebiasaan individu. Fitur numerik meliputi age, avg_glucose_level, dan bmi—meskipun bmi memiliki beberapa nilai yang hilang (missing values) sebanyak 201 data. Sedangkan hypertension, heart_disease, dan stroke adalah kolom bertipe integer yang menunjukkan status hipertensi, penyakit jantung, dan kejadian stroke (target). Dataset ini siap untuk diproses lebih lanjut dalam pembangunan model klasifikasi risiko stroke dengan mempertimbangkan penanganan nilai yang hilang dan pengkodean fitur kategorikal."""

print(f'Jumlah missing value: {df.isnull().sum()}')

"""Terdapat missing value di bmi sebanyak 201 data"""

print(f'Jumlah baris duplikat: {df.duplicated().sum()}')

"""tidak ada data duplikat"""

df.describe()

"""Terdapat nilai yang tidak logis yaitu 0.08 tahun (sekitar 29 hari), sehingga bisa dianggap sebagai outlier dan perlu dihapus"""

# Define the list of numerical columns
selected_features = ['age', 'avg_glucose_level', 'bmi','hypertension', 'heart_disease']

# Matriks korelasi
corr_matrix = df[selected_features].corr()

# Visualisasi heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title("Matriks Korelasi 5 Fitur Numerik")
plt.show()

"""- age – bmi (0.33): Korelasi positif sedang; semakin tua usia, cenderung BMI juga semakin tinggi.
- age – hypertension (0.28): Korelasi positif lemah; usia yang lebih tua cenderung memiliki risiko hipertensi lebih tinggi.
- age – heart_disease (0.26): Korelasi positif lemah; peningkatan usia sedikit berkaitan dengan risiko penyakit jantung.
- age – avg_glucose_level (0.24): Korelasi positif lemah; glukosa rata-rata sedikit meningkat seiring bertambahnya usia.
-bmi – avg_glucose_level (0.18): Korelasi sangat lemah; hubungan hampir tidak signifikan antara BMI dan kadar glukosa.
- bmi – hypertension (0.17): Korelasi sangat lemah; orang dengan BMI tinggi sedikit cenderung memiliki hipertensi.
- avg_glucose_level – hypertension (0.17): Korelasi sangat lemah; hubungan sangat kecil antara kadar glukosa dan hipertensi.
- avg_glucose_level – heart_disease (0.16): Korelasi sangat lemah; hampir tidak ada hubungan antara glukosa dan penyakit jantung.
- bmi – heart_disease (0.04): Korelasi hampir tidak ada; BMI tidak berhubungan signifikan dengan penyakit jantung.
- hypertension – heart_disease (0.11): Korelasi sangat lemah; terdapat sedikit hubungan antara hipertensi dan penyakit jantung.

# Data Preparation
"""

# Drop ID column
df.drop('id', axis=1, inplace=True)
df

"""Menghapus ID karena itu tidak ada hubungannya dengan data stroke"""

df = df.dropna()
print(f'Jumlah missing value setelah dihapus: {df.isnull().sum()}')

"""menghapus data missing value"""

# Menghapus nilai usia tidak logis
df = df[df['age'] >= 1]

"""menghapus usia dibawah 1 tahun"""

# Encoding Categorical Variables
le = LabelEncoder()
for col in ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']:
    df[col] = le.fit_transform(df[col])
df

"""  Kolom kategorikal seperti gender, ever_married, work_type, Residence_type, dan smoking_status diubah menjadi angka menggunakan LabelEncoder. Model machine learning umumnya hanya dapat memproses data numerik. Encoding mengubah string menjadi representasi numerik sehingga dapat digunakan dalam model."""

X = df.drop(columns='stroke')  # Fitur
y = df['stroke']               # Target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

"""Memisahkan data menjadi dua bagian: data pelatihan (80%) dan data pengujian (20%) dengan train_test_split dari sklearn. Model perlu diuji pada data yang belum pernah dilihat untuk mengetahui seberapa baik kemampuannya melakukan generalisasi."""

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""Melakukan standardisasi pada fitur numerik menggunakan StandardScaler, agar semua fitur memiliki distribusi dengan mean 0 dan standar deviasi 1.Beberapa algoritma seperti K-Nearest Neighbors (KNN) atau algoritma berbasis jarak sangat dipengaruhi oleh skala data. Fitur dengan skala lebih besar dapat mendominasi hasil.

# Modeling

K-NN
"""

# KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
knn_preds = knn.predict(X_test)

"""- `knn = KNeighborsClassifier(n_neighbors=5)` Membuat objek model KNN dengan parameter n_neighbors=5, yang artinya model akan memprediksi label berdasarkan 5 tetangga terdekat.
- `knn.fit(X_train, y_train)`Melatih model KNN menggunakan data latih X_train (fitur) dan y_train (label/target). Proses pelatihan ini menyimpan data latih karena KNN adalah model berbasis instance (lazy learning), tidak membangun model eksplisit saat training
- `knn_preds = knn.predict(X_test)` Menggunakan model KNN untuk memprediksi label pada data uji X_test, dengan membandingkan jarak data uji ke data latih dan mengambil mayoritas label dari 5 tetangga terdekat. Hasil prediksi disimpan di variabel knn_preds.

"""

# Decision Tree
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)
dt_preds = dt.predict(X_test)

"""- `from sklearn.tree import DecisionTreeClassifier`  Mengimpor kelas DecisionTreeClassifier dari scikit-learn. Model ini digunakan untuk klasifikasi berbasis Desicion tree.
- `dt = DecisionTreeClassifier(random_state=42)`  Membuat objek model Decision Tree dengan seed acak 42 agar hasil konsisten.
- `dt.fit(X_train, y_train)`  Melatih model dengan data latih X_train dan y_train.
- `dt_preds = dt.predict(X_test)` Menghasilkan prediksi label dari data uji X_test.
"""

# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
rf_preds = rf.predict(X_test)

"""- `rf = RandomForestClassifier(n_estimators=100, random_state=42)`  Membuat objek model Random Forest dengan parameter:
     - `n_estimators=100` berarti model akan menggunakan 100 pohon keputusan dalam ensemble-nya (semakin banyak, semakin stabil prediksi, tapi lebih lambat).
     - random_state=42 adalah angka acak tetap untuk reproducibility — agar hasilnya selalu sama setiap kali dijalankan.
- `rf.fit(X_train, y_train)` Melatih (training) model Random Forest menggunakan data latih X_train (fitur) dan y_train (label/target). Model akan mempelajari pola hubungan antara fitur dan target dari data tersebut.
- `rf_preds = rf.predict(X_test)` Menggunakan model yang telah dilatih untuk memprediksi data uji (X_test), hasil prediksi disimpan dalam variabel rf_preds, yang berisi kelas/label hasil prediksi untuk masing-masing baris dalam X_test.

# EVALUASI
"""

# Buat ulang fungsi evaluasi untuk train dan test
def get_metrics(name, model, X_train, y_train, X_test, y_test):
    train_preds = model.predict(X_train)
    test_preds = model.predict(X_test)

    return {
        "Model": name,
        "Train Accuracy": accuracy_score(y_train, train_preds),
        "Test Accuracy": accuracy_score(y_test, test_preds),
        "Train Precision": precision_score(y_train, train_preds, zero_division=0),
        "Test Precision": precision_score(y_test, test_preds, zero_division=0),
        "Train Recall": recall_score(y_train, train_preds, zero_division=0),
        "Test Recall": recall_score(y_test, test_preds, zero_division=0),
        "Train F1": f1_score(y_train, train_preds, zero_division=0),
        "Test F1": f1_score(y_test, test_preds, zero_division=0)
    }

# Dapatkan metrik evaluasi
evals = [
    get_metrics("KNN", knn, X_train, y_train, X_test, y_test),
    get_metrics("Decision Tree", dt, X_train, y_train, X_test, y_test),
    get_metrics("Random Forest", rf, X_train, y_train, X_test, y_test)
]

# Ubah ke DataFrame
eval_df_full = pd.DataFrame(evals)
display(eval_df_full)

"""Decision Tree lebih baik dibanding KNN dan Random Forest untuk kasus ini karena masih memberikan nilai precision, recall, dan F1 score di data testing, walaupun performanya masih rendah secara absolut.
KNN dan Random Forest gagal sama sekali dalam mendeteksi kelas positif pada data testing (precision dan recall = 0).
"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# List model dan prediksi
models = [("KNN", knn_preds), ("Decision Tree", dt_preds), ("Random Forest", rf_preds)]

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for ax, (title, preds) in zip(axes, models):
    cm = confusion_matrix(y_test, preds)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)
    ax.set_title(f'Confusion Matrix - {title}')
    ax.set_xlabel('Predicted')
    ax.set_ylabel('Actual')

plt.tight_layout()
plt.show()

"""Berdasarkan hasil visualisasi confusion matrix, model KNN menghasilkan 0 True Positive (TP), 54 False Negative (FN), 918 True Negative (TN), dan 2 False Positive (FP). Model Decision Tree menunjukkan 10 TP, 44 FN, 887 TN, dan 33 FP. Sementara itu, model Random Forest memiliki 0 TP, 54 FN, 920 TN, dan 0 FP. Nilai-nilai ini mencerminkan bagaimana masing-masing model melakukan klasifikasi terhadap kelas positif dan negatif pada data pengujian.

#  feature importance
"""

# Daftar fitur yang digunakan (tanpa 'id' dan 'stroke')
fitur = ['gender', 'age', 'hypertension', 'heart_disease', 'ever_married',
         'work_type', 'Residence_type', 'avg_glucose_level', 'bmi', 'smoking_status']

# Menampilkan feature importance Decision Tree
dt_importance = pd.DataFrame({
    'Fitur': fitur,
    'Importance': dt.feature_importances_
}).sort_values(by='Importance', ascending=False)
print("Decision Tree Feature Importance:")
print(dt_importance)

plt.figure(figsize=(8,5))
plt.barh(dt_importance['Fitur'], dt_importance['Importance'], color='lightgreen')
plt.gca().invert_yaxis()
plt.title('Feature Importance - Decision Tree')
plt.xlabel('Importance')
plt.show()

"""Kode tersebut menampilkan tingkat pentingnya masing-masing fitur yang digunakan oleh model Decision Tree dalam proses pengambilan keputusan untuk memprediksi risiko stroke. Pertama, dibuat sebuah DataFrame yang berisi daftar fitur (selain kolom 'id' dan 'stroke') dan nilai feature_importances_ yang dihasilkan oleh model Decision Tree setelah pelatihan, yang menunjukkan kontribusi relatif tiap fitur terhadap hasil prediksi. DataFrame ini kemudian diurutkan berdasarkan nilai pentingnya secara menurun agar fitur yang paling berpengaruh berada di atas. Selanjutnya, visualisasi berupa grafik batang horizontal dibuat untuk mempermudah interpretasi, dengan fitur yang paling penting tampil di bagian atas grafik, sehingga memudahkan analisis dalam memahami fitur mana yang paling signifikan dalam menentukan risiko stroke menurut model tersebut. Berdasarkan hasil fitur yang berpengaruh di decision three adalah glukosa, bmi dan umur"""

# Menampilkan feature importance Random Forest
rf_importance = pd.DataFrame({
    'Fitur': fitur,
    'Importance': rf.feature_importances_
}).sort_values(by='Importance', ascending=False)
print("Random Forest Feature Importance:")
print(rf_importance)

plt.figure(figsize=(8,5))
plt.barh(rf_importance['Fitur'], rf_importance['Importance'], color='skyblue')
plt.gca().invert_yaxis()
plt.title('Feature Importance - Random Forest')
plt.xlabel('Importance')
plt.show()

"""Kode ini berfungsi untuk menampilkan dan memvisualisasikan tingkat pentingnya masing-masing fitur dalam model Random Forest yang digunakan untuk memprediksi risiko stroke. Pertama, dibuat sebuah DataFrame yang berisi daftar fitur (yang sudah didefinisikan sebelumnya) beserta nilai feature_importances_ dari model Random Forest setelah proses pelatihan, yang menunjukkan seberapa besar kontribusi tiap fitur dalam menentukan hasil prediksi. DataFrame tersebut kemudian diurutkan berdasarkan nilai pentingnya secara menurun agar fitur dengan pengaruh terbesar berada di atas. Selanjutnya, dibuat grafik batang horizontal berwarna biru muda (skyblue) untuk memudahkan pemahaman visual mengenai fitur mana saja yang paling berpengaruh dalam model. Grafik tersebut dibalik sumbunya agar fitur terpenting tampil di bagian atas, membantu analisis lebih cepat terhadap faktor-faktor kunci risiko stroke menurut model Random Forest.  Berdasarkan hasil fitur yang berpengaruh di Random Forest adalah glukosa, bmi dan umur"""
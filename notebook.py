# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CkfbRTbu6ffOaPddVooTn5SZJcd41jbt

SUBMISSION PREDICTIVE ANALYTYC - PREDIKSI STROKE

original dataset: https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset

# Import Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from IPython.display import display
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

"""Mengimport semua library yang dibutuhkan

# Load Data
"""

df = pd.read_csv('healthcare-dataset-stroke-data.csv')
df

"""Menampilkan seluruh data, data diawal terdapat 5110 data

# Exploratory Data Analysis (EDA)
"""

df.info()

"""Dataset ini berisi 5110 entri dengan 12 kolom yang merepresentasikan data kesehatan dan gaya hidup individu terkait risiko stroke. Kolom id merupakan identifier unik setiap data, sementara gender, ever_married, work_type, Residence_type, dan smoking_status adalah fitur kategorikal bertipe objek yang menggambarkan karakteristik demografis dan kebiasaan individu. Fitur numerik meliputi age, avg_glucose_level, dan bmi—meskipun bmi memiliki beberapa nilai yang hilang (missing values) sebanyak 201 data. Sedangkan hypertension, heart_disease, dan stroke adalah kolom bertipe integer yang menunjukkan status hipertensi, penyakit jantung, dan kejadian stroke (target). Dataset ini siap untuk diproses lebih lanjut dalam pembangunan model klasifikasi risiko stroke dengan mempertimbangkan penanganan nilai yang hilang dan pengkodean fitur kategorikal."""

print(f'Jumlah missing value: {df.isnull().sum()}')

"""Terdapat missing value di bmi sebanyak 201 data"""

# Drop ID column
df.drop('id', axis=1, inplace=True)
df

"""Menghapus ID karena itu tidak ada hubungannya dengan data stroke"""

print(f'Jumlah baris duplikat: {df.duplicated().sum()}')

"""tidak ada data duplikat"""

df.describe()

"""Terdapat nilai yang tidak logis yaitu 0.08 tahun (sekitar 29 hari), sehingga bisa dianggap sebagai outlier dan perlu dihapus"""

# Define the list of numerical columns
selected_features = ['age', 'avg_glucose_level', 'bmi','hypertension', 'heart_disease']

# Matriks korelasi
corr_matrix = df[selected_features].corr()

# Visualisasi heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title("Matriks Korelasi 5 Fitur Numerik")
plt.show()

"""- age – bmi (0.33): Korelasi positif sedang; semakin tua usia, cenderung BMI juga semakin tinggi.
- age – hypertension (0.28): Korelasi positif lemah; usia yang lebih tua cenderung memiliki risiko hipertensi lebih tinggi.
- age – heart_disease (0.26): Korelasi positif lemah; peningkatan usia sedikit berkaitan dengan risiko penyakit jantung.
- age – avg_glucose_level (0.24): Korelasi positif lemah; glukosa rata-rata sedikit meningkat seiring bertambahnya usia.
-bmi – avg_glucose_level (0.18): Korelasi sangat lemah; hubungan hampir tidak signifikan antara BMI dan kadar glukosa.
- bmi – hypertension (0.17): Korelasi sangat lemah; orang dengan BMI tinggi sedikit cenderung memiliki hipertensi.
- avg_glucose_level – hypertension (0.17): Korelasi sangat lemah; hubungan sangat kecil antara kadar glukosa dan hipertensi.
- avg_glucose_level – heart_disease (0.16): Korelasi sangat lemah; hampir tidak ada hubungan antara glukosa dan penyakit jantung.
- bmi – heart_disease (0.04): Korelasi hampir tidak ada; BMI tidak berhubungan signifikan dengan penyakit jantung.
- hypertension – heart_disease (0.11): Korelasi sangat lemah; terdapat sedikit hubungan antara hipertensi dan penyakit jantung.

# Data Preparation
"""

df = df.dropna()
print(f'Jumlah missing value setelah dihapus: {df.isnull().sum()}')

"""menghapus data missing value"""

# Menghapus nilai usia tidak logis
df = df[df['age'] >= 1]

"""menghapus usia dibawah 1 tahun"""

# Encoding Categorical Variables
le = LabelEncoder()
for col in ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']:
    df[col] = le.fit_transform(df[col])
df

"""  Kolom kategorikal seperti gender, ever_married, work_type, Residence_type, dan smoking_status diubah menjadi angka menggunakan LabelEncoder. Model machine learning umumnya hanya dapat memproses data numerik. Encoding mengubah string menjadi representasi numerik sehingga dapat digunakan dalam model."""

X = df.drop(columns='stroke')  # Fitur
y = df['stroke']               # Target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

"""Memisahkan data menjadi dua bagian: data pelatihan (80%) dan data pengujian (20%) dengan train_test_split dari sklearn. Model perlu diuji pada data yang belum pernah dilihat untuk mengetahui seberapa baik kemampuannya melakukan generalisasi."""

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""Melakukan standardisasi pada fitur numerik menggunakan StandardScaler, agar semua fitur memiliki distribusi dengan mean 0 dan standar deviasi 1.Beberapa algoritma seperti K-Nearest Neighbors (KNN) atau algoritma berbasis jarak sangat dipengaruhi oleh skala data. Fitur dengan skala lebih besar dapat mendominasi hasil.

# Modeling

K-NN
"""

# KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
knn_preds = knn.predict(X_test)

"""- `knn = KNeighborsClassifier(n_neighbors=5)` Membuat objek model KNN dengan parameter n_neighbors=5, yang artinya model akan memprediksi label berdasarkan 5 tetangga terdekat.
- `knn_preds = knn.predict(X_test)` Melakukan prediksi pada data uji X_test menggunakan model KNN yang sudah dilatih.
"""

# Decision Tree
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)
dt_preds = dt.predict(X_test)

"""- `from sklearn.tree import DecisionTreeClassifier`  Mengimpor kelas DecisionTreeClassifier dari scikit-learn. Model ini digunakan untuk klasifikasi berbasis Desicion tree.
- `dt = DecisionTreeClassifier(max_depth=10, random_state=42)`  Membuat objek model Decision Tree dengan kedalaman maksimum pohon 10 dan seed acak 42 agar hasil konsisten.
- `dt.fit(X_train, y_train)`  Melatih model dengan data latih X_train dan y_train.
- `dt_preds = dt.predict(X_test)` Menghasilkan prediksi label dari data uji X_test.
"""

# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
rf_preds = rf.predict(X_test)

"""- `from sklearn.ensemble import RandomForestClassifier` Mengimpor kelas RandomForestClassifier dari pustaka scikit-learn
- `rf = RandomForestClassifier(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)`  Membuat objek model Random Forest dengan parameter:
   - `n_estimators=50`: Jumlah pohon keputusan yang dibuat.
   - `max_depth=16`: Membatasi kedalaman maksimal setiap pohon.
   - `random_state=55`: Untuk hasil yang konsisten.
   - `n_jobs=-1`: Gunakan semua core CPU agar pelatihan lebih cepat.
 - `rf.fit(X_train, y_train)` Melatih model menggunakan data pelatihan.
 - `rf_preds = rf.predict(X_test)` Menghasilkan prediksi pada data uji.

# EVALUASI
"""

# Buat ulang fungsi evaluasi untuk train dan test
def get_metrics(name, model, X_train, y_train, X_test, y_test):
    train_preds = model.predict(X_train)
    test_preds = model.predict(X_test)

    return {
        "Model": name,
        "Train Accuracy": accuracy_score(y_train, train_preds),
        "Test Accuracy": accuracy_score(y_test, test_preds),
        "Train Precision": precision_score(y_train, train_preds, zero_division=0),
        "Test Precision": precision_score(y_test, test_preds, zero_division=0),
        "Train Recall": recall_score(y_train, train_preds, zero_division=0),
        "Test Recall": recall_score(y_test, test_preds, zero_division=0),
        "Train F1": f1_score(y_train, train_preds, zero_division=0),
        "Test F1": f1_score(y_test, test_preds, zero_division=0)
    }

# Dapatkan metrik evaluasi
evals = [
    get_metrics("KNN", knn, X_train, y_train, X_test, y_test),
    get_metrics("Decision Tree", dt, X_train, y_train, X_test, y_test),
    get_metrics("Random Forest", rf, X_train, y_train, X_test, y_test)
]

# Ubah ke DataFrame
eval_df_full = pd.DataFrame(evals)
display(eval_df_full)

"""Decision Tree lebih baik dibanding KNN dan Random Forest untuk kasus ini karena masih memberikan nilai precision, recall, dan F1 score di data testing, walaupun performanya masih rendah secara absolut.
KNN dan Random Forest gagal sama sekali dalam mendeteksi kelas positif pada data testing (precision dan recall = 0).
"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# List model dan prediksi
models = [("KNN", knn_preds), ("Decision Tree", dt_preds), ("Random Forest", rf_preds)]

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for ax, (title, preds) in zip(axes, models):
    cm = confusion_matrix(y_test, preds)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)
    ax.set_title(f'Confusion Matrix - {title}')
    ax.set_xlabel('Predicted')
    ax.set_ylabel('Actual')

plt.tight_layout()
plt.show()

"""Berdasarkan hasil visualisasi confusion matrix, model KNN menghasilkan 0 True Positive (TP), 54 False Negative (FN), 918 True Negative (TN), dan 2 False Positive (FP). Model Decision Tree menunjukkan 10 TP, 44 FN, 887 TN, dan 33 FP. Sementara itu, model Random Forest memiliki 0 TP, 54 FN, 920 TN, dan 0 FP. Nilai-nilai ini mencerminkan bagaimana masing-masing model melakukan klasifikasi terhadap kelas positif dan negatif pada data pengujian."""